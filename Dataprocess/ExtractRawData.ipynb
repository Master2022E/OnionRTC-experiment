{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Extract Raw report data from ObserveRTC\n",
    "\n",
    "The output is a series of CSV files, one for each client type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import helperFunctions as hf\n",
    "\n",
    "hf.setup()\n",
    "\n",
    "outputFolder = \"output_folder/rawReport/\"\n",
    "if not os.path.exists(outputFolder):\n",
    "   os.makedirs(outputFolder)\n",
    "   logging.info(f\"The directory \\\"{outputFolder}\\\" is created!\")\n",
    "\n",
    "client = MongoClient(hf.getConnectionString())\n",
    "database=client[\"observertc-reports\"]\n",
    "calls = database[\"calls\"]\n",
    "reports = database[\"reports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 13:53:03 INFO     Start extracting Unique Calls And Outcomes from the calls database. \n",
      "2023-01-31 13:53:03 INFO     Shape:  (19222, 7) \n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start extracting Unique Calls And Outcomes from the calls database.\")\n",
    "\n",
    "query = { \"logging_type\": {\"$in\" : [\"COMMAND_SESSION_SUCCESS\", \"COMMAND_SESSION_FAILURE\", \"COMMAND_SESSION_FAILED_SETUP\"]}}\n",
    "          \n",
    "df = pd.DataFrame(calls.find(query))\n",
    "\n",
    "logging.info(f\"Shape:  {df.shape}\")\n",
    "\n",
    "df.to_csv(\"output_folder/uniqueCallsAndOutcomes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31 13:53:03 INFO     Starting query for all clients \n",
      "2023-01-31 13:53:03 INFO     Starting query for c1-Normal \n",
      "2023-01-31 13:53:03 INFO     Got data for c1-Normal. Converting to data frame. \n",
      "2023-01-31 13:55:03 INFO     c1-Normal, Dataset complete, size: 508639, converting types \n",
      "2023-01-31 13:55:26 INFO     c1-Normal, Dataset converted, writing to file \n",
      "2023-01-31 13:55:40 INFO     Starting query for c2-TorNormal \n",
      "2023-01-31 13:55:40 INFO     Got data for c2-TorNormal. Converting to data frame. \n",
      "2023-01-31 13:56:08 INFO     c2-TorNormal, Dataset complete, size: 417036, converting types \n",
      "2023-01-31 13:56:20 INFO     c2-TorNormal, Dataset converted, writing to file \n",
      "2023-01-31 13:56:32 INFO     Starting query for c3-TorEurope \n",
      "2023-01-31 13:56:32 INFO     Got data for c3-TorEurope. Converting to data frame. \n",
      "2023-01-31 13:57:01 INFO     c3-TorEurope, Dataset complete, size: 414471, converting types \n",
      "2023-01-31 13:57:15 INFO     c3-TorEurope, Dataset converted, writing to file \n",
      "2023-01-31 13:57:38 INFO     Starting query for c4-TorScandinavia \n",
      "2023-01-31 13:57:38 INFO     Got data for c4-TorScandinavia. Converting to data frame. \n",
      "2023-01-31 13:58:21 INFO     c4-TorScandinavia, Dataset complete, size: 284606, converting types \n",
      "2023-01-31 13:58:29 INFO     c4-TorScandinavia, Dataset converted, writing to file \n",
      "2023-01-31 13:58:39 INFO     Starting query for c5-I2P \n",
      "2023-01-31 13:58:39 INFO     Got data for c5-I2P. Converting to data frame. \n",
      "2023-01-31 13:58:44 INFO     c5-I2P, Dataset complete, size: 0, converting types \n",
      "2023-01-31 13:58:44 INFO     Starting query for c6-Lokinet \n",
      "2023-01-31 13:58:44 INFO     Got data for c6-Lokinet. Converting to data frame. \n",
      "2023-01-31 13:59:01 INFO     c6-Lokinet, Dataset complete, size: 243252, converting types \n",
      "2023-01-31 13:59:06 INFO     c6-Lokinet, Dataset converted, writing to file \n",
      "2023-01-31 13:59:13 INFO     Starting query for d1-Normal \n",
      "2023-01-31 13:59:13 INFO     Got data for d1-Normal. Converting to data frame. \n",
      "2023-01-31 13:59:49 INFO     d1-Normal, Dataset complete, size: 486445, converting types \n",
      "2023-01-31 14:00:11 INFO     d1-Normal, Dataset converted, writing to file \n",
      "2023-01-31 14:00:26 INFO     Starting query for d2-TorNormal \n",
      "2023-01-31 14:00:26 INFO     Got data for d2-TorNormal. Converting to data frame. \n",
      "2023-01-31 14:00:55 INFO     d2-TorNormal, Dataset complete, size: 389110, converting types \n",
      "2023-01-31 14:01:13 INFO     d2-TorNormal, Dataset converted, writing to file \n",
      "2023-01-31 14:01:23 INFO     Starting query for d3-TorEurope \n",
      "2023-01-31 14:01:23 INFO     Got data for d3-TorEurope. Converting to data frame. \n",
      "2023-01-31 14:01:55 INFO     d3-TorEurope, Dataset complete, size: 389045, converting types \n",
      "2023-01-31 14:02:11 INFO     d3-TorEurope, Dataset converted, writing to file \n",
      "2023-01-31 14:02:22 INFO     Starting query for d4-TorScandinavia \n",
      "2023-01-31 14:02:22 INFO     Got data for d4-TorScandinavia. Converting to data frame. \n",
      "2023-01-31 14:02:46 INFO     d4-TorScandinavia, Dataset complete, size: 355568, converting types \n",
      "2023-01-31 14:02:54 INFO     d4-TorScandinavia, Dataset converted, writing to file \n",
      "2023-01-31 14:03:05 INFO     Starting query for d5-I2P \n",
      "2023-01-31 14:03:05 INFO     Got data for d5-I2P. Converting to data frame. \n",
      "2023-01-31 14:03:09 INFO     d5-I2P, Dataset complete, size: 0, converting types \n",
      "2023-01-31 14:03:09 INFO     Starting query for d6-Lokinet \n",
      "2023-01-31 14:03:09 INFO     Got data for d6-Lokinet. Converting to data frame. \n",
      "2023-01-31 14:03:26 INFO     d6-Lokinet, Dataset complete, size: 243581, converting types \n",
      "2023-01-31 14:03:30 INFO     d6-Lokinet, Dataset converted, writing to file \n"
     ]
    }
   ],
   "source": [
    "# Run through the Reports collection.\n",
    "\n",
    "logging.info(f\"Starting query for all clients\")\n",
    "for userId in hf.userIds:\n",
    "\n",
    "    logging.info(f\"Starting query for {userId}\")\n",
    "    \n",
    "    query = {   \"type\": \"CLIENT_EXTENSION_DATA\", \n",
    "                \"payload.extensionType\" : \n",
    "                    { \"$in\" : [ \"OUT_BOUND_RTC\", \n",
    "                                \"IN_BOUND_RTC\", \n",
    "                                \"REMOTE_OUT_BOUND_RTC\", \n",
    "                                \"REMOTE_IN_BOUND_RTC\"]},\n",
    "                \"payload.userId\": userId}\n",
    "\n",
    "    cursor = reports.find(query)\n",
    "\n",
    "    logging.info(f\"Got data for {userId}. Converting to data frame.\")\n",
    "\n",
    "    dataSet = []\n",
    "    for record in cursor:\n",
    "        data = {}\n",
    "        data[\"timestamp\"] = record[\"payload\"][\"timestamp\"]\n",
    "        data[\"callId\"] = record[\"payload\"][\"callId\"]\n",
    "        data[\"roomId\"] = record[\"payload\"][\"roomId\"]\n",
    "        data[\"clientId\"] = record[\"payload\"][\"clientId\"]\n",
    "        data[\"userId\"] = record[\"payload\"][\"userId\"]\n",
    "        data[\"sampleSeq\"] = record[\"payload\"][\"sampleSeq\"]\n",
    "\n",
    "        a = json.loads(record[\"payload\"][\"payload\"])\n",
    "        # https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\n",
    "        data = {**data, **a[\"stats\"]}\n",
    "\n",
    "        dataSet.append(data)\n",
    "\n",
    "\n",
    "    logging.info(f\"{userId}, Dataset complete, size: {len(dataSet)}, converting types\")\n",
    "    if(len(dataSet) > 0):\n",
    "        newData = pd.DataFrame(dataSet)\n",
    "        newData[\"callId\"]=newData[\"callId\"].astype(str)\n",
    "        newData[\"roomId\"]=newData[\"roomId\"].astype(str)\n",
    "        newData[\"clientId\"]=newData[\"clientId\"].astype(str)\n",
    "        newData[\"userId\"]=newData[\"userId\"].astype(str)\n",
    "        logging.info(f\"{userId}, Dataset converted, writing to file\")\n",
    "        newData.to_csv((outputFolder + userId + \".csv\") , mode='w', header=True, index=False)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "\n",
     "\n",
     "\n"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "735cbb64c6f33b2d3af11b1f5c3709c6b8b8827e91f2b991e330f5d3f3bd728b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
