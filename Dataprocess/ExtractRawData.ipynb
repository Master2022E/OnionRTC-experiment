{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Extract Raw report data from ObserveRTC\n",
    "\n",
    "The output is a series of CSV files, one for each client type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "  format=f'%(asctime)s %(levelname)-8s %(message)s ',\n",
    "  level=logging.INFO,\n",
    "  datefmt='%Y-%m-%d %H:%M:%S',\n",
    "  handlers=[\n",
    "  logging.StreamHandler(sys.stdout)\n",
    "  ])\n",
    "\n",
    "outputFolder = \"output_folder/rawReport/\"\n",
    "\n",
    "\n",
    "if not os.path.exists(outputFolder):\n",
    "\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(outputFolder)\n",
    "   logging.info(f\"The directory \\\"{outputFolder}\\\" is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-26 11:42:25 INFO     Starting query for user: c1-Normal \n",
      "2023-01-26 11:42:25 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:42:51 INFO     c1-Normal, Dataset complete, size: 508639, coverting types \n",
      "2023-01-26 11:43:04 INFO     c1-Normal, Dataset converted, writing to file \n",
      "2023-01-26 11:43:17 INFO     Starting query for user: c2-TorNormal \n",
      "2023-01-26 11:43:17 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:43:42 INFO     c2-TorNormal, Dataset complete, size: 417036, coverting types \n",
      "2023-01-26 11:43:54 INFO     c2-TorNormal, Dataset converted, writing to file \n",
      "2023-01-26 11:44:05 INFO     Starting query for user: c3-TorEurope \n",
      "2023-01-26 11:44:05 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:44:30 INFO     c3-TorEurope, Dataset complete, size: 414471, coverting types \n",
      "2023-01-26 11:44:37 INFO     c3-TorEurope, Dataset converted, writing to file \n",
      "2023-01-26 11:44:48 INFO     Starting query for user: c4-TorScandinavia \n",
      "2023-01-26 11:44:48 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:45:05 INFO     c4-TorScandinavia, Dataset complete, size: 284606, coverting types \n",
      "2023-01-26 11:45:09 INFO     c4-TorScandinavia, Dataset converted, writing to file \n",
      "2023-01-26 11:45:16 INFO     Starting query for user: c5-I2P \n",
      "2023-01-26 11:45:16 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:45:20 INFO     c5-I2P, Dataset complete, size: 0, coverting types \n",
      "2023-01-26 11:45:20 INFO     Starting query for user: c6-Lokinet \n",
      "2023-01-26 11:45:20 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:45:34 INFO     c6-Lokinet, Dataset complete, size: 243252, coverting types \n",
      "2023-01-26 11:45:38 INFO     c6-Lokinet, Dataset converted, writing to file \n",
      "2023-01-26 11:45:44 INFO     Starting query for user: d1-Normal \n",
      "2023-01-26 11:45:44 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:46:11 INFO     d1-Normal, Dataset complete, size: 486445, coverting types \n",
      "2023-01-26 11:46:18 INFO     d1-Normal, Dataset converted, writing to file \n",
      "2023-01-26 11:46:31 INFO     Starting query for user: d2-TorNormal \n",
      "2023-01-26 11:46:31 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:46:53 INFO     d2-TorNormal, Dataset complete, size: 389110, coverting types \n",
      "2023-01-26 11:46:59 INFO     d2-TorNormal, Dataset converted, writing to file \n",
      "2023-01-26 11:47:09 INFO     Starting query for user: d3-TorEurope \n",
      "2023-01-26 11:47:09 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:47:31 INFO     d3-TorEurope, Dataset complete, size: 389045, coverting types \n",
      "2023-01-26 11:47:37 INFO     d3-TorEurope, Dataset converted, writing to file \n",
      "2023-01-26 11:47:47 INFO     Starting query for user: d4-TorScandinavia \n",
      "2023-01-26 11:47:47 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:48:08 INFO     d4-TorScandinavia, Dataset complete, size: 355568, coverting types \n",
      "2023-01-26 11:48:13 INFO     d4-TorScandinavia, Dataset converted, writing to file \n",
      "2023-01-26 11:48:22 INFO     Starting query for user: d5-I2P \n",
      "2023-01-26 11:48:22 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:48:26 INFO     d5-I2P, Dataset complete, size: 0, coverting types \n",
      "2023-01-26 11:48:27 INFO     Starting query for user: d6-Lokinet \n",
      "2023-01-26 11:48:27 INFO     Got data from database, converting to data frame. \n",
      "2023-01-26 11:48:40 INFO     d6-Lokinet, Dataset complete, size: 243581, coverting types \n",
      "2023-01-26 11:48:44 INFO     d6-Lokinet, Dataset converted, writing to file \n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "address = 'mongodb://{user}:{password}@{host}:{port}'.format(\n",
    "    user= os.getenv('MONGO_USER'),\n",
    "    password= os.getenv('MONGO_PASSWORD'),\n",
    "    host= \"localhost\",\n",
    "    port= os.getenv('MONGO_PORT')\n",
    ")\n",
    "client = MongoClient(address)\n",
    "database=client[\"observertc-reports\"]\n",
    "reportsDatabase = database.reports\n",
    "\n",
    "\n",
    "userIds = [\n",
    "    \"c1-Normal\",\n",
    "    \"c2-TorNormal\",\n",
    "    \"c3-TorEurope\",\n",
    "    \"c4-TorScandinavia\",\n",
    "    \"c5-I2P\",\n",
    "    \"c6-Lokinet\",\n",
    "    \"d1-Normal\",\n",
    "    \"d2-TorNormal\",\n",
    "    \"d3-TorEurope\",\n",
    "    \"d4-TorScandinavia\",\n",
    "    \"d5-I2P\",\n",
    "    \"d6-Lokinet\"\n",
    "]\n",
    "\n",
    "for userId in userIds:\n",
    "    \n",
    "\n",
    "    logging.info(f\"Starting query for user: {userId}\")\n",
    "\n",
    "    query = {   \"type\": \"CLIENT_EXTENSION_DATA\", \n",
    "                \"payload.extensionType\" : \n",
    "                    { \"$in\" : [ \"OUT_BOUND_RTC\", \n",
    "                                \"IN_BOUND_RTC\", \n",
    "                                \"REMOTE_OUT_BOUND_RTC\", \n",
    "                                \"REMOTE_IN_BOUND_RTC\"]},\n",
    "                \"payload.userId\": userId}\n",
    "\n",
    "    #logging.info(f\"Query: {query}\")\n",
    "\n",
    "    cursor = reportsDatabase.find(query)\n",
    "    ##size = len(list(cursor))\n",
    "    #logging.info(f\"Got data from database, size: {size}, converting to data frame.\")\n",
    "    logging.info(f\"Got data from database, converting to data frame.\")\n",
    "\n",
    "    dataSet = []\n",
    "    #i = 0\n",
    "    for record in cursor:\n",
    "        \n",
    "        data = {}\n",
    "        #append timestamp to data\n",
    "        data[\"timestamp\"] = record[\"payload\"][\"timestamp\"]\n",
    "        data[\"callId\"] = record[\"payload\"][\"callId\"]\n",
    "        data[\"roomId\"] = record[\"payload\"][\"roomId\"]\n",
    "        data[\"clientId\"] = record[\"payload\"][\"clientId\"]\n",
    "        data[\"userId\"] = record[\"payload\"][\"userId\"]\n",
    "        data[\"sampleSeq\"] = record[\"payload\"][\"sampleSeq\"]\n",
    "\n",
    "        a = json.loads(record[\"payload\"][\"payload\"])\n",
    "\n",
    "        # https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\n",
    "        data = {**data, **a[\"stats\"]}\n",
    "\n",
    "\n",
    "\n",
    "        #if(i % 1000 == 0):\n",
    "        #    logging.info(f\"Processed {i} records.\")\n",
    "        dataSet.append(data)\n",
    "        #i = i + 1\n",
    "        \n",
    "\n",
    "        #i = i+1\n",
    "\n",
    "    logging.info(f\"{userId}, Dataset complete, size: {len(dataSet)}, coverting types\")\n",
    "    if(len(dataSet) > 0):\n",
    "        newData = pd.DataFrame(dataSet)\n",
    "        newData[\"callId\"]=newData[\"callId\"].astype(str)\n",
    "        newData[\"roomId\"]=newData[\"roomId\"].astype(str)\n",
    "        newData[\"clientId\"]=newData[\"clientId\"].astype(str)\n",
    "        newData[\"userId\"]=newData[\"userId\"].astype(str)\n",
    "        logging.info(f\"{userId}, Dataset converted, writing to file\")\n",
    "        newData.to_csv((outputFolder + userId + \".csv\") , mode='w', header=True, index=False)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "\n",
     "\n",
     "\n"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "735cbb64c6f33b2d3af11b1f5c3709c6b8b8827e91f2b991e330f5d3f3bd728b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
