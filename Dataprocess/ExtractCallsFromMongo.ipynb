{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Extract Raw Data from ObserveRTC\n",
    "\n",
    "The output is a CSV file which will be appened to after each run.\n",
    "\n",
    "A local file will keep track of when the script was run last and only query data for that given time period.\n",
    "\n",
    "If there already exists a local data file, then it will load it and remove the latest duplicates. \n",
    "\n",
    "And append the newest entries. After this other data processing can happen by reading the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "fileOutput = \"rawCallData.csv\"\n",
    "fileTimeOfLastQuery = \"lastCallsQueryTime.txt\"\n",
    "\n",
    "address = 'mongodb://{user}:{password}@localhost:{port}'.format(\n",
    "    user= os.getenv('MONGO_USER'),\n",
    "    password= os.getenv('MONGO_PASSWORD'),\n",
    "    host= os.getenv('MONGO_HOST'),\n",
    "    port= os.getenv('MONGO_PORT')\n",
    ")\n",
    "client = MongoClient(address)\n",
    "database=client[\"observertc-reports\"]\n",
    "\n",
    "collection=database[\"calls\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTimeOfLastQuery() -> int:\n",
    "  timeOfLastQuery = 0\n",
    "  try:\n",
    "    with open(fileTimeOfLastQuery, \"r\") as f:\n",
    "      timeOfLastQuery = int(f.read()) \n",
    "      print(\"Last run on:\", datetime.fromtimestamp(timeOfLastQuery/1000), \"going to search for new data 1 hour before this.\" )\n",
    "      timeOfLastQuery -= 1000 * 60 * 60 # 1 hour before last query\n",
    "      return timeOfLastQuery\n",
    "  except Exception as e:\n",
    "      pass\n",
    "  return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTimeOfLastQuery(timeOfLastQuery: int):\n",
    "  with open(fileTimeOfLastQuery, \"w\") as f:\n",
    "    print(\"Saving timeOfCurrentQuery:\", timeOfLastQuery, \"as epoch:\", int(timeOfLastQuery * 1000))\n",
    "    f.write(str(int(timeOfLastQuery * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JTT\\AppData\\Local\\Temp\\ipykernel_29300\\125480495.py:1: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  testdata = pd.read_csv(fileOutput)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_id                 object\n",
       "client_username     object\n",
       "client_id           object\n",
       "client_type         object\n",
       "room_id             object\n",
       "test_id             object\n",
       "logging_type        object\n",
       "state               object\n",
       "timestamp           object\n",
       "iplocation          object\n",
       "latest_circuit      object\n",
       "error               object\n",
       "scenario_type      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = pd.read_csv(fileOutput)\n",
    "testdata.head()\n",
    "\n",
    "# print column types\n",
    "testdata.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on: 2022-12-19 15:01:04 going to search for new data 1 hour before this.\n",
      "Searching for records since: 2022-12-19 14:01:04\n",
      "Got data from database, converting to data frame\n",
      "Trying to read and load from rawCallData.csv\n",
      "size of old data:  (33865, 13)\n",
      "size of new data:  (34102, 13)\n",
      "merging data\n",
      "size of merge:  (34102, 26)\n",
      "filtering merge to diff\n",
      "size of diff:  (34102, 26)\n"
     ]
    }
   ],
   "source": [
    "#read number from text file\n",
    "timeOfLastQuery = readTimeOfLastQuery()\n",
    "print(\"Searching for records since:\", datetime.fromtimestamp(timeOfLastQuery/1000))\n",
    "\n",
    "timeOfCurrentQuery = datetime.now()\n",
    "cursor = collection.find()\n",
    "\n",
    "print(\"Got data from database, converting to data frame\")\n",
    "dataSet = []\n",
    "for record in cursor:\n",
    "    #record.pop(\"_id\")\n",
    "    dataSet.append(record)\n",
    "\n",
    "\n",
    "\n",
    "newData = pd.DataFrame(dataSet)\n",
    "#print(\"columns in new data: \", newData.columns)\n",
    "newData[\"client_username\"] = newData[\"client_username\"].astype(object)\n",
    "newData[\"client_id\"] = newData[\"client_id\"].astype(object)\n",
    "newData[\"client_type\"] = newData[\"client_type\"].astype(object)\n",
    "newData[\"room_id\"] = newData[\"room_id\"].astype(object)\n",
    "newData[\"test_id\"] = newData[\"test_id\"].astype(object)\n",
    "newData[\"logging_type\"] = newData[\"logging_type\"].astype(object)\n",
    "newData[\"state\"] = newData[\"state\"].astype(object)\n",
    "newData[\"iplocation\"] = newData[\"state\"].astype(object)\n",
    "newData[\"latest_circuit\"] = newData[\"state\"].astype(object)\n",
    "newData[\"error\"] = newData[\"state\"].astype(object)            \n",
    "newData[\"scenario_type\"] = newData[\"scenario_type\"].astype(object)\n",
    "\n",
    "# set timestamp as time index\n",
    "\n",
    "#\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"Trying to read and load from\", fileOutput)\n",
    "\n",
    "    \n",
    "    oldData = pd.read_csv(fileOutput)\n",
    "\n",
    "    oldData[\"client_username\"] = oldData[\"client_username\"].astype(object)\n",
    "    oldData[\"client_id\"] = oldData[\"client_id\"].astype(object)\n",
    "    oldData[\"client_type\"] = oldData[\"client_type\"].astype(object)\n",
    "    oldData[\"room_id\"] = oldData[\"room_id\"].astype(object)\n",
    "    oldData[\"test_id\"] = oldData[\"test_id\"].astype(object)\n",
    "    oldData[\"logging_type\"] = oldData[\"logging_type\"].astype(object)\n",
    "    oldData[\"state\"] = oldData[\"state\"].astype(object)\n",
    "    oldData[\"iplocation\"] = oldData[\"state\"].astype(object)\n",
    "    oldData[\"latest_circuit\"] = oldData[\"state\"].astype(object)\n",
    "    oldData[\"error\"] = oldData[\"state\"].astype(object)            \n",
    "    oldData[\"scenario_type\"] = oldData[\"scenario_type\"].astype(object)\n",
    "\n",
    "\n",
    "    print(\"size of old data: \", oldData.shape)\n",
    "    print(\"size of new data: \", newData.shape)\n",
    "\n",
    "    on = ['_id', 'client_username', 'client_id', 'client_type', 'room_id',\n",
    "       'test_id', 'logging_type', 'state', 'timestamp', 'iplocation',\n",
    "       'latest_circuit', 'error', 'scenario_type']\n",
    "\n",
    "    print(\"merging data\")\n",
    "\n",
    "    merge = pd.merge(oldData, newData, indicator=True, on=on, how='right')\n",
    "    print(\"size of merge: \", merge.shape)\n",
    "\n",
    "    #print(\"head of merge\")\n",
    "    #print(merge.head(1))\n",
    "\n",
    "    #print(\"tail of merge\")\n",
    "    #print(merge.tail(1))\n",
    "\n",
    "    print(\"filtering merge to diff\")\n",
    "    #diff = merge.query('_merge!=\"both\"').drop('_merge', axis=1)\n",
    "    diff = merge\n",
    "    print(\"size of diff: \", diff.shape)\n",
    "\n",
    "    # TODO: check if diff is actually the diff and not just a complete copy of newData.\n",
    "    raise Exception(\"The merge is not working as expected, so I'm stopping the script here.\")\n",
    "\n",
    "    # append diff to data.csv\n",
    "    #diff.to_csv(fileOutput, mode='a', header=False, index=False)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error reading from file:\", e)\n",
    "    print(\"size of old data: \", 0)\n",
    "    print(\"size of new data: \", newData.shape)\n",
    "    print(\"size of diff: \", newData.shape)\n",
    "    newData.to_csv(fileOutput, mode='w', header=True, index=False)\n",
    "    saveTimeOfLastQuery(int(timeOfCurrentQuery.timestamp()))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Other error:\", e)\n",
    "    print(e.with_traceback)\n",
    "    print(e.args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "\n",
     "\n",
     "\n"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "735cbb64c6f33b2d3af11b1f5c3709c6b8b8827e91f2b991e330f5d3f3bd728b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
